---
title: "Simulation5.1_RF"
author: "Maja GlediÄ‡"
date: "2025-06-28"
output: pdf_document
---


This is the code for performing cross-validation where the nuisance models 
are defined as a random forest (RF) propensity score model.
All steps remain the same as the baseline model (logistic regression), only difference is '
the type of nuisance model used. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# list of required packages
packages_needed <- c("dplyr", "ggplot2", "pbapply", "randomForest")

# install any missing packages and load libraries
for (pkg in packages_needed) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, dependencies = TRUE)
    library(pkg, character.only = TRUE)
  } else {
    library(pkg, character.only = TRUE)
  }
}
```

# Generate Data
```{r generate data}

sim_data <- function(n, seed) {
  set.seed(seed)
  
  # covariates -> assume all are observed confounders 
  x1 <- rnorm(n, mean = 0, sd = 1)  
  x2 <- 0.6*x1 + rnorm(n, mean = 0, sd = 0.5)  
  x3 <- rbinom(n, 1, 0.5)                          
  x4 <- sample(1:3, n, replace = TRUE)           
  x5 <- 0.4*x1 + 0.3*x2 + rnorm(n, mean = 0, sd = 0.5)  
  x6 <- 0.5*x5 - 0.2*x3 + rnorm(n, mean = 0, sd = 0.5)
  
  # probability of receiving treatment
  prob_a <- plogis(-0.175 + 0.1*x1 + 0.3*x2 + 0.2*x3 - 0.01*(x4 == 2) + 0.02*x5 - 0.3*x6)
  a <- rbinom(n, 1, prob_a)   # (1 = treated, 0 = untreated)
  # hist(prob_a)

  # counterfactual / potential outcomes
  y_a1 <- rbinom(n, 1, plogis(-1.2 + 0.005*x1 + 0.003*x2^2 + 0.004*x5 - 0.002*x6 + 0.002*x3^3 - 0.004*(x4 == 3) + 0.0025*1))  # treated
  y_a0 <- rbinom(n, 1, plogis(-1.2 + 0.005*x1 + 0.003*x2^2 + 0.004*x5 - 0.002*x6 + 0.002*x3^3 - 0.004*(x4 == 3) + 0.0025*0))  # untreated
  
  # observed outcome based on treatment received 
  y_obs <- ifelse(a == 1, y_a1, y_a0)
  
  df <- data.frame(X1 = x1, X2 = x2, X3 = as.factor(x3), 
                   X4 = as.factor(x4), X5 = x5, X6 = x6, 
                   A = as.factor(a), 
                   Y_a0 = as.factor(y_a0),
                   Y_a1 = as.factor(y_a1),
                   Y_obs = as.factor(y_obs))
  
  return(df)
}

```

# Cross-validation Procedure
```{r cross-validation, warning=FALSE}

# create function to run 5-fold cross-validation
# define the function to run 'r' replications for a given sample size 'n',
# with 'k' number of folds


cv_manual <- function(n, r, k_folds = 5) {
    
  # run simulated data
  df <- sim_data(n, seed = 123 + r)
  # partition data into 5 folds
  folds <- cut(sample(nrow(df)), breaks = k_folds, labels = FALSE)
  
  # initialise storage of Brier scores across 5 folds
  true_brier_A0 <- numeric(k_folds)
  true_brier_A1 <- numeric(k_folds)
  est_brier1_A0 <- numeric(k_folds)
  est_brier1_A1 <- numeric(k_folds)
  est_brier2_A0 <- numeric(k_folds)
  est_brier2_A1 <- numeric(k_folds)
  est_brier3_A0 <- numeric(k_folds)
  est_brier3_A1 <- numeric(k_folds)
  est_brier4_A0 <- numeric(k_folds)
  est_brier4_A1 <- numeric(k_folds)
              
  # for loop for cross-validation
    for (i in 1:k_folds){
      
      # ----------------
      # PREDICTION MODEL
      # ----------------'
      
      # assign train and validation sets such that the split is 80/20
      train <- df[folds != i, ]
      test <- df[folds == i, ]
      
      # NUISANCE MODEL 1: random forest propensity score model
      # develop propensity score model from train data and determine weights
      ps_null <- randomForest(A ~ X1 + X2 + X3 + X4 + X5 + X6, 
                 # mtry = 1, # introduces randomness; 
                              # tests model under extreme simplicity, 
                              # of variables to split on
                 ntree = 100, # standard default, quick to train
                 # nodesize = 5, # obs per terminal node, 
                                 # prevents tiny, overconfident leaves 
                                 # with extreme probabilities
                 # maxnodes = 30, # prevents overly complex trees and 
                                  # limits splits globally
                 data = train)
          
      # estimate propensity scores for the training set
      train$ps_null <- predict(ps_null, type = "prob")[,2]
      # get inverse probability weights for the training set
      train$weights_null <- ifelse(train$A == 1, 1/train$ps_null, 1/(1-train$ps_null)) 
      
      # some infinite weights were estimated
      # decided to truncate weights using 99th percentile 
      # as upper bound if weight = Inf
      train$weights_null_trunc <- ifelse(train$weights_null == Inf, 
                                    pmin(train$weights_null, quantile(train$weights_null, 0.98)), 
                                         train$weights_null)
      
      # train$weights_null == train$weights_null_trunc   
      
      # create weighted prediction model by including the training set weights
      model_null <- glm(Y_obs ~ A + X1 + X2 + X3 + X4 + X5 + X6, data = train, 
                        family = "binomial", weights =  train$weights_null_trunc)
      
      # the test set will be used for making predictions
      # need to create scenarios so that for all observations A={0,1}
      # create two test sets and make A={0,1} for all
      test_A0 <- test
      test_A0$A <- as.factor(0)
      test_A1 <- test 
      test_A1$A <- as.factor(1)
      
      # get predictions from test sets (a=0, a=1) and attach to original test set
      test$predsA0 <- predict(model_null, newdata = test_A0, type = "response")  
      test$predsA1 <- predict(model_null, newdata = test_A1, type = "response") 
      
      # ----------------
      # TRUE BRIER SCORE
      # ----------------
      
      # extract true counterfactuals from test set based on treatment assignment
      test_Y_a0 <- as.numeric(test$Y_a0) - 1
      test_Y_a1 <- as.numeric(test$Y_a1) - 1
      
      # evaluate performance for each iteration:
      # compare true counterfactuals with prediction probabilities
      true_brier_A0[i] <- mean((test_Y_a0 - test$predsA0)^2)   
      true_brier_A1[i] <- mean((test_Y_a1 - test$predsA1)^2)
      
      # ---------------------
      # ESTIMATED BRIER SCORE
      # ---------------------
      
      test$Y_obs <- as.numeric(test$Y_obs) - 1
      
      # --------------------------------------------------
      # STRATEGY 1: develop nuisance model using FULL DATA
      # --------------------------------------------------
      
      # STEP 1: train propensity score model using full data
      ps_model1 <- randomForest(A ~ X1 + X2 + X3 + X4 + X5 + X6, 
                    ntree = 100, data = df) # mtry = 1, nodesize = 5, maxnodes = 30
      
      # STEP 2: estimate propensity scores for the validation set
      test$ps1 <- predict(ps_model1, newdata = test, type = "prob")[,2]  
      
      # STEP 3: get weights for validation set
      test$weights1 <- ifelse(test$A == 1, 1 / test$ps1, 1 / (1 - test$ps1))
      # check for Inf weights and truncate if necessary
      test$weights1_trunc <- ifelse(test$weights1 == Inf, 
                                    pmin(test$weights1, quantile(test$weights1, 0.99)), 
                                    test$weights1) # check statement 
      # test$weights1 == test$weights1_trunc
      
      # extract Y_obs from test set based on treatment assignment
      Yobs_test_A0 <- as.numeric(test[test$A==0, "Y_obs"]) - 1    
      Yobs_test_A1 <- as.numeric(test[test$A==1, "Y_obs"]) - 1   
      
      # do not subset - removing observations does not take into account number of 0s
      # have zero contribution which changes mean (number of obs less)
    
      # STEP 4: compute estimated Brier scores for each iteration:
      # using observed outcomes, predicted probabilities from prediction model,
      # and weights from nuisance model 2
      est_brier1_A0[i] <- mean(test$weights1_trunc * (test$A==0) * (test$Y_obs - test$predsA0)^2) 
      est_brier1_A1[i] <- mean(test$weights1_trunc * (test$A==1) * (test$Y_obs - test$predsA1)^2)
    
      # -----------------------------------------------------
      # STRATEGY 2: develop nuisance model using TRAINING SET
      # -----------------------------------------------------
      
      # STEP 1: train propensity score using training set
      # Repeat steps 2-4 
      ps_model2 <- randomForest(A ~ X1 + X2 + X3 + X4 + X5 + X6, 
                     ntree = 100, data = train) # mtry = 1, nodesize = 5, maxnodes = 30,  
      test$ps2 <- predict(ps_model2, newdata = test, type = "prob")[,2]  
      test$weights2 <- ifelse(test$A == 1, 1 / test$ps2, 1 / (1 - test$ps2))
      test$weights2_trunc <- ifelse(test$weights2 == Inf, 
                                    pmin(test$weights2, quantile(test$weights2, 0.99)), 
                                    test$weights2)
      # test$weights2 == test$weights2_trunc
      
      est_brier2_A0[i] <- mean(test$weights2_trunc * (test$A==0)* (test$Y_obs - test$predsA0)^2)
      est_brier2_A1[i] <- mean(test$weights2_trunc * (test$A==1)* (test$Y_obs - test$predsA1)^2) 
      
      # -------------------------------------------------------
      # STRATEGY 3: develop nuisance model using VALIDATION SET
      # -------------------------------------------------------
      
      # STEP 1: train propensity score using test set
      # Repeat steps 2-4 
      ps_model3 <- randomForest(A ~ X1 + X2 + X3 + X4 + X5 + X6, 
                     ntree = 100, data = test) # mtry = 1, nodesize = 5, maxnodes = 30, 
      test$ps3 <- predict(ps_model3, newdata = test, type = "prob")[,2]
      test$weights3 <- ifelse(test$A == 1, 1 / test$ps3, 1 / (1 - test$ps3))
      test$weights3_trunc <- ifelse(test$weights3 == Inf, 
                                    pmin(test$weights3, quantile(test$weights3, 0.99)), 
                                    test$weights3)
      # test$weights3 == test$weights3_trunc

      
      est_brier3_A0[i] <- mean(test$weights3_trunc * (test$A==0) * (test$Y_obs - test$predsA0)^2) 
      est_brier3_A1[i] <- mean(test$weights3_trunc * (test$A==1) * (test$Y_obs - test$predsA1)^2)
    
      # -----------------------------------------------------
      # STRATEGY 4: develop nuisance model using NUISANCE SET
      # -----------------------------------------------------
  
    
      # STEP 1: split test into 2 parts - nuisance and remainder of test set
      # we partition the test set such that it is a 50/50 split
      idx <- sample(nrow(test), size = round(0.5*nrow(test)), replace = FALSE) 
      nuisance <- test[idx,]
      remaining <- test[-idx,]
      
      # develop nuisance model using nuisance set
      ps_model4 <- randomForest(A ~ X1 + X2 + X3 + X4 + X5 + X6, 
                     ntree = 100, data = nuisance) # mtry = 1, nodesize = 5, maxnodes = 30, 
    
      # STEP 3: estimate propensity scores on the remaining validation set
      remaining$ps4 <- predict(ps_model4, newdata = remaining, type = "prob")[,2]
      # STEP 4: get the weights for the remaining test (validation set)
      remaining$weights4 <- ifelse(remaining$A == 1, 1 / remaining$ps4, 1 / (1 - remaining$ps4))
      remaining$weights4_trunc <- ifelse(remaining$weights4 == Inf, 
                                    pmin(remaining$weights4, quantile(remaining$weights4, 0.99)), 
                                    remaining$weights4)
      # test$weights4 == test$weights4_trunc
      
      # STEP 5: estimate Brier scores accordingly
      est_brier4_A0[i] <- mean(remaining$weights4_trunc * (remaining$A==0) * (remaining$Y_obs - remaining$predsA0)^2) 
      est_brier4_A1[i] <- mean(remaining$weights4_trunc * (remaining$A==1) * (remaining$Y_obs - remaining$predsA1)^2)
    }
    
    # average the Brier scores across the 5-folds
    data.frame(replication = r,
                sample_size = n,  
                True_A0 = mean(true_brier_A0),
                True_A1 = mean(true_brier_A1),
                Est1_A0 = mean(est_brier1_A0),
                Est1_A1 = mean(est_brier1_A1),
                Est2_A0 = mean(est_brier2_A0),
                Est2_A1 = mean(est_brier2_A1),
                Est3_A0 = mean(est_brier3_A0),
                Est3_A1 = mean(est_brier3_A1),
                Est4_A0 = mean(est_brier4_A0),
                Est4_A1 = mean(est_brier4_A1))
}
```

```{r analysis checks}
# check propensity scores and weights  

# mean(test$ps1)
# mean(test$ps2)
# mean(test$ps3)
# mean(as.numeric(test$A)-1)
# 
# mean(remaining$ps4)
# mean(as.numeric(remaining$A)-1)
# 
# hist(test$weights1)
# hist(test$weights2)
# hist(test$ps1)
# hist(test$ps2)
# ggplot(data = remaining, mapping = aes(x = ps4, fill = A)) +
#   geom_histogram(bins = 10)
# 
# ggplot(data = test, mapping = aes(x = ps3, fill = A)) +
#   geom_histogram(bins = 10)

```


## Replications and Sample Sizes
```{r replicate simulation}
n_reps <- 100
sample_sizes <- c(500)

for (n in sample_sizes) {
  
  # run cross-validation for all replications per sample size
  results_cv_list <- pblapply(1:n_reps, function(r) {
    cv_manual(n = n, r = r)
  })
  
  # combine results and save as .rda for each sample size
  results_cv_rf <- do.call(rbind, results_cv_list)
  save(results_cv_rf, file = paste0("results_RF_", n, ".rda"))
  
  message("Finished and saved: Sample Size = ", n)
}

load("results_RF_500.rda")
results_cv_rf
```


## Loading the saved data files
```{r load files}

sample_sizes <- c(500, 1000, 1e4, 1e5)

# load the files for each sample size
loaded_results <- lapply(sample_sizes, function(n) {
  
  file_name <- paste0("results_RF_", n, ".rda")
  data <- get(load(file_name))
  return(data)
})

# combine all datasets into one dataframe
combined_results_rf <- do.call(rbind, loaded_results)
```


# Summary
## Untreated Group

### Distribution of Brier scores
```{r distribution of Brier scores}
# subset such that we only have a=0
# transform into long format 
long_df0_rf <- combined_results_rf %>%
  dplyr::select(replication, sample_size, True_A0, Est1_A0, Est2_A0, Est3_A0, Est4_A0) %>%
  tidyr::pivot_longer(cols = starts_with("True_A0"):starts_with("Est4_A0"), 
               names_to = "strategy", values_to = "brier_score")

# rename to display strategy names
long_df0_rf <- long_df0_rf %>% mutate(strategy = recode(strategy,
                           "True_A0" = "True",
                           "Est1_A0" = "Full_Data",
                           "Est2_A0" = "Training_Set",
                           "Est3_A0" = "Test_Set",
                           "Est4_A0" = "Nuisance_Set")) %>%
            mutate(strategy = factor(strategy, 
                           levels = c("True","Full_Data", 
                                      "Training_Set", "Test_Set", 
                                      "Nuisance_Set")))

#-------------------------------------------------------------------------------

# Boxplot: distribution of average estimates from 100 replications
boxplot0_rf <- ggplot(long_df0_rf, 
                      aes(x = strategy, y = brier_score, 
                          fill = strategy, color = strategy)) +
  
  geom_jitter(alpha = 0.5) +
  geom_boxplot(alpha = 0.7, color = "black") +
  
  labs(# title = "Distribution of Brier Scores (A=0)",
       # subtitle = "Random Forest Nuisance Model", 
       # caption = "Average estimates from 1000 replications",
       y = "Average Brier Score", x = "") +
  
  scale_color_manual(values = c("True" = "#F8766D",
                                "Full_Data" = "#CD9600", 
                                "Training_Set" = "#00C19A", 
                                "Test_Set" = "#00A9FF",
                                "Nuisance_Set" = "#ED68ED"), 
                     name = "Strategy") +
  
  scale_fill_manual(values = c("True" = "#F8766D",
                                "Full_Data" = "#CD9600", 
                                "Training_Set" = "#00C19A", 
                                "Test_Set" = "#00A9FF",
                                "Nuisance_Set" = "#ED68ED"), 
                    name = "Strategy") +

  theme_minimal() +
  
  theme(axis.text.x = element_blank()) +
  
  facet_wrap(~sample_size) +
  
  scale_y_continuous(limits = c(0.10, 0.26)) 


# ggsave("boxplot0_rf.png", plot = boxplot0_rf)
```


### Average Brier scores 
```{r average Brier scores}
# subset Brier scores such that we only have for A=0
results_A0 <- combined_results_rf %>% 
  dplyr::select(replication, sample_size, 
                True_A0, Est1_A0, Est2_A0, Est3_A0, Est4_A0)

# remove NAs and Infs (this removed 3 rows)
results_A0 <- results_A0[is.finite(rowSums(results_A0)),]

# compute the mean and standard deviations for each score
# across the simulation runs
summary_stats_A0 <- results_A0 %>%
  group_by(sample_size) %>%
  summarise(across(.cols = c(True_A0, Est1_A0, Est2_A0, Est3_A0, Est4_A0),
                   .fns = list(mean_brier = mean, sd_brier = sd),
                   .names = "{.col}_{.fn}"))


# transform the dataframe to separate values based on strategies 
summary_A0 <- summary_stats_A0 %>%
  tidyr::pivot_longer(
    cols = -sample_size, 
    names_to = c("strategy", "metric"), 
    names_sep = "_A0_") %>%
  tidyr::pivot_wider(names_from = metric,
                     values_from = value) %>%
  mutate(strategy = recode(strategy,
                           "True" = "True",
                           "Est1" = "Full_Data",
                           "Est2" = "Training_Set",
                           "Est3" = "Test_Set",
                           "Est4" = "Nuisance_Set")) %>%
  mutate(strategy = factor(strategy, 
                           levels = c("True","Full_Data", 
                                      "Training_Set", "Test_Set", 
                                      "Nuisance_Set")))
```


```{r plot averages}
mean_briers0_rf <- ggplot(data = summary_A0, 
       mapping = aes(x = as.factor(sample_size),
                     y = mean_brier, 
                     color = strategy, 
                     shape = strategy, 
                     linetype = strategy, 
                     group = strategy)) +
  
  geom_point(size = 3) +
  geom_line(linewidth = 0.8) +
  
  scale_x_discrete(breaks = c(500, 1e3, 1e4, 1e5)) +

  # geom_errorbar(aes(ymin = mean_brier - sd_brier,
                #     ymax = mean_brier + sd_brier), 
                # width= 0.1) +
  
  scale_color_manual(values = c("True" = "#F8766D",
                                "Full_Data" = "#CD9600", 
                                "Training_Set" = "#00C19A", 
                                "Test_Set" = "#00A9FF",
                                "Nuisance_Set" = "#ED68ED")) + 

  
  scale_shape_manual(values = c("True" = 8,
                                "Full_Data" = 15,
                                "Training_Set" = 16,
                                "Test_Set" = 17,
                                "Nuisance_Set" = 18)) +

  scale_linetype_manual(values = c( "True" = "solid",
                                    "Full_Data" = "dashed",
                                   "Training_Set" = "dotted",
                                   "Test_Set" = "dotdash",
                                   "Nuisance_Set" = "longdash")) +
  
  guides(color = guide_legend("Strategy"), shape = guide_legend("Strategy"), linetype = "none") +
  
  scale_y_continuous(limits = c(0.10, 0.24)) +


  labs(x = "Sample Size", y = "Mean Brier Score") +
       #title = "Average Brier Score (A=0)") +
  
  theme_minimal()

# ggsave("mean_briers0_rf.png", plot = mean_briers0_rf)


```

### Overall Performance Measures
Here we compute the bias, variance and RMSE related to the Brier score estimates.

```{r performance measures}
# extract the true values per sample size
true_values <- summary_A0 %>% filter(strategy == "True") %>%
  dplyr::select(sample_size, true_brier = mean_brier)

# join true values to all rows
# compute bias, variance, and RMSE
sum_all_A0 <- summary_A0 %>%
  left_join(true_values, by = "sample_size") %>%
  filter(strategy != "True") %>%
  mutate(bias = mean_brier - true_brier,
         variance = sd_brier^2,
         rmse = sqrt(bias^2 + variance)) %>%
  mutate(strategy = factor(strategy, 
                           levels = c("Full_Data", "Training_Set", 
                                      "Test_Set", "Nuisance_Set")))

# select relevant columns
table_RF_A0 <- sum_all_A0 %>%
  select(sample_size, strategy, bias, variance, rmse)

# ggplot(data = sum_all_A0,
#        aes(x = as.factor(sample_size),
#            y = bias,
#            color = strategy,
#            shape = strategy,
#            linetype = strategy,
#            group = strategy)) +
# 
#   geom_point(size = 3) +
#   geom_line(linewidth = 0.8) +
#   scale_y_continuous(limits = c(-0.0001, 0.05))
# 
# 
# ggplot(data = sum_all_A0,
#        aes(x = as.factor(sample_size),
#            y = variance,
#            color = strategy,
#            shape = strategy,
#            linetype = strategy,
#            group = strategy)) +
# 
#   geom_point(size = 3) +
#   geom_line(linewidth = 0.8) +
#   scale_y_continuous(limits = c(-0.0001, 0.0005))
# 
# ggplot(data = sum_all_A0,
#        aes(x = as.factor(sample_size),
#            y = rmse,
#            color = strategy,
#            shape = strategy,
#            linetype = strategy,
#            group = strategy)) +
# 
#   geom_point(size = 3) +
#   geom_line(linewidth = 0.8) +
#   scale_y_continuous(limits = c(0.01, 0.08))

```


## Treated Group (a=1)
Performing same as above, but with the treatment group (a=1).
### Distribution of Brier scores
```{r}
# transform into long format 
long_df1_rf <- combined_results_rf %>%
  dplyr::select(replication, sample_size, True_A1, Est1_A1, Est2_A1, Est3_A1, Est4_A1) %>%
  tidyr::pivot_longer(cols = starts_with("True_A1"):starts_with("Est4_A1"), 
               names_to = "strategy", values_to = "brier_score")

long_df1_rf <- long_df1_rf %>% mutate(strategy = recode(strategy,
                           "True_A1" = "True",
                           "Est1_A1" = "Full_Data",
                           "Est2_A1" = "Training_Set",
                           "Est3_A1" = "Test_Set",
                           "Est4_A1" = "Nuisance_Set")) %>%
            mutate(strategy = factor(strategy, 
                           levels = c("True","Full_Data", 
                                      "Training_Set", "Test_Set", 
                                      "Nuisance_Set")))

#-------------------------------------------------------------------------------

# Boxplot: distribution of average estimates from 100 replications
boxplot1_rf <- ggplot(long_df1_rf, 
                      aes(x = strategy, y = brier_score, 
                          fill = strategy, color = strategy)) +
  
  geom_jitter(alpha = 0.5) +
  geom_boxplot(alpha = 0.7, color = "black") +
  
  labs(# title = "Distribution of Brier Scores (A=1)",
       # subtitle = "Logistic Regression Nuisance Model", 
       # caption = "Average estimates from 1000 replications",
       y = "Average Brier Score", x = "") +
  
  scale_color_manual(values = c("True" = "#F8766D",
                                "Full_Data" = "#CD9600", 
                                "Training_Set" = "#00C19A", 
                                "Test_Set" = "#00A9FF",
                                "Nuisance_Set" = "#ED68ED"), 
                     name = "Strategy") +
  
  scale_fill_manual(values = c("True" = "#F8766D",
                                "Full_Data" = "#CD9600", 
                                "Training_Set" = "#00C19A", 
                                "Test_Set" = "#00A9FF",
                                "Nuisance_Set" = "#ED68ED"), 
                    name = "Strategy") +

  theme_minimal() +
  
  theme(axis.text.x = element_blank()) +
  
  facet_wrap(~sample_size) +
  
  scale_y_continuous(limits = c(0.10, 0.26)) 


# ggsave("boxplot1_rf.png", plot = boxplot1_rf)
```


### Average Scores
```{r}
results_A1 <- combined_results_rf %>% dplyr::select(replication, sample_size, 
                                 True_A1, Est1_A1, Est2_A1, Est3_A1, Est4_A1)

# remove NAs and Infs (this removed 3 rows)
results_A1 <- results_A1[is.finite(rowSums(results_A1)),]


summary_stats_A1 <- results_A1 %>%
  group_by(sample_size) %>%
  summarise(across(.cols = c(True_A1, Est1_A1, Est2_A1, Est3_A1, Est4_A1),
                   .fns = list(mean_brier = mean, sd_brier = sd),
                   .names = "{.col}_{.fn}"))

summary_A1 <- summary_stats_A1 %>%
  tidyr::pivot_longer(
    cols = -sample_size, 
    names_to = c("strategy", "metric"), 
    names_sep = "_A1_") %>%
  tidyr::pivot_wider(names_from = metric,
                     values_from = value) %>%
  mutate(strategy = recode(strategy,
                           "True" = "True",
                           "Est1" = "Full_Data",
                           "Est2" = "Training_Set",
                           "Est3" = "Test_Set",
                           "Est4" = "Nuisance_Set")) %>%
  mutate(strategy = factor(strategy, 
                           levels = c("True","Full_Data", 
                                      "Training_Set", "Test_Set", 
                                      "Nuisance_Set")))
```


```{r}
mean_briers1_rf <- ggplot(data = summary_A1, 
       mapping = aes(x = as.factor(sample_size),
                     y = mean_brier, 
                     color = strategy, 
                     shape = strategy, 
                     linetype = strategy, 
                     group = strategy)) +
  
  geom_point(size = 3) +
  geom_line(linewidth = 0.8) +
  
  scale_x_discrete(breaks = c(500, 1e3, 1e4, 1e5)) +

  # geom_errorbar(aes(ymin = mean_brier - sd_brier,
                #     ymax = mean_brier + sd_brier), 
                # width= 0.1) +
  
  scale_color_manual(values = c("True" = "#F8766D",
                                "Full_Data" = "#CD9600", 
                                "Training_Set" = "#00C19A", 
                                "Test_Set" = "#00A9FF",
                                "Nuisance_Set" = "#ED68ED")) + 

  
  scale_shape_manual(values = c("True" = 8,
                                "Full_Data" = 15,
                                "Training_Set" = 16,
                                "Test_Set" = 17,
                                "Nuisance_Set" = 18)) +

  scale_linetype_manual(values = c( "True" = "solid",
                                    "Full_Data" = "dashed",
                                   "Training_Set" = "dotted",
                                   "Test_Set" = "dotdash",
                                   "Nuisance_Set" = "longdash")) +
  
  guides(color = guide_legend("Strategy"), shape = guide_legend("Strategy"), linetype = "none") +
  

  labs(x = "Sample Size", y = "Mean Brier Score") +
       #title = "Average Brier Score (A=1)") +
  
  scale_y_continuous(limits = c(0.10, 0.26)) +
  
  theme_minimal()


# ggsave("mean_briers1_rf.png", plot = mean_briers1_rf)

```


### Overall Performance Measures
#### Bias, Variance, RMSE
```{r}
# extract the true values per sample size
true_values <- summary_A1 %>% filter(strategy == "True") %>%
  dplyr::select(sample_size, true_brier = mean_brier)

# join true values to all rows
sum_all_A1 <- summary_A1 %>%
  left_join(true_values, by = "sample_size") %>%
  filter(strategy != "True") %>%
  mutate(bias = mean_brier - true_brier,
         variance = sd_brier^2,
         rmse = sqrt(bias^2 + variance)) %>%
  mutate(strategy = factor(strategy, 
                           levels = c("Full_Data", "Training_Set", 
                                      "Test_Set", "Nuisance_Set")))

# select relevant columns
table_RF_A1 <- sum_all_A1 %>%
  select(sample_size, strategy, bias, variance, rmse)
```

